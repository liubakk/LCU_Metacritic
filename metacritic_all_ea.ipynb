{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600102920481",
   "display_name": "Python 3.7.4 64-bit ('anaconda3': virtualenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "--- 30.67937207221985 seconds ---\n"
    }
   ],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "import numpy as np \n",
    "\n",
    "# OPTIMIZATION : Calculating the start time\n",
    "start_time = time.time()\n",
    "\n",
    "#Dictionary for titles urls scraping\n",
    "title_url_dict= {'title':[]}\n",
    "\n",
    "#Generating the title list from the 'Electronic Arts' page on metacritic\n",
    "user_agent = {'User-agent': 'Mozilla/5.0'}\n",
    "url = 'https://www.metacritic.com/company/electronic-arts'\n",
    "response  = requests.get(url, headers = user_agent)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "#current page cout 58 (30 items per page)\n",
    "page_count = int(soup.find('li', class_='page last_page').find('a').text)\n",
    "\n",
    "for page in range (0,page_count):\n",
    "        url = 'https://www.metacritic.com/company/electronic-arts?page='+str(page)\n",
    "        response  = requests.get(url, headers = user_agent)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        for title in soup.find_all('td', class_='title brief_metascore'):\n",
    "                if title.find('a') == None:\n",
    "                        break\n",
    "                # CLEANUP: Removing mobile titles\n",
    "                if '/ios/' not in title.find('a')['href']:\n",
    "                        title_url_dict['title'].append(title.find('a')['href'])\n",
    "        \n",
    "        # PROGRESSLINE : % of pages processed - printed in the output bar\n",
    "        clear_output()\n",
    "        print(\"Progress: {:2.1%}\".format(page / page_count))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "#converting the dictionary to a dataframe\n",
    "title_url_df = pd.DataFrame(title_url_dict)\n",
    "\n",
    "#generating the titles urls array \n",
    "title_url_list = title_url_df['title'].to_numpy()\n",
    "\n",
    "#generating the platforms and titles names arrays\n",
    "title_url_df = title_url_df.title.str.split(\"/\",expand=True,)\n",
    "platforms = title_url_df[2].to_numpy()\n",
    "titles = title_url_df[3].to_numpy()\n",
    "\n",
    "# PROGRESSLINE : Completed - printed in the output bar\n",
    "clear_output()\n",
    "#print(\"Complete!\")\n",
    "\n",
    "# OPTIMIZATION : Calculating the time elapsed\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_integer(dt_time):\n",
    "    return 10000*dt_time.year + 100*dt_time.month + dt_time.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "--- 889.2345888614655 seconds ---\n"
    }
   ],
   "source": [
    "# OPTIMIZATION : Calculating the start time\n",
    "start_time = time.time()\n",
    "\n",
    "user_agent = {'User-agent': 'Mozilla/5.0'}\n",
    "\n",
    "#Dictionary for titles info scraping\n",
    "ea_title_dict = {'DATE_ID':[], 'PUBLISHER_URL':[], 'TITLE_URL':[], 'TITLE_NAME':[], 'PLATFORM':[],'TITLE_GENRES':[], 'RELEASE_DATE_ID':[],\n",
    "                'METASCORE':[], 'CRITIC_REVIEWS_COUNT':[],'POS_CRITIC_REVIEWS_COUNT':[],'MIX_CRITIC_REVIEWS_COUNT':[],'NEG_CRITIC_REVIEWS_COUNT':[],\n",
    "                'AGV_USER_SCORE':[], 'USER_RATINGS_COUNT':[],'POS_USER_REVIEWS_COUNT':[],'MIX_USER_REVIEWS_COUNT':[],'NEG_USER_REVIEWS_COUNT':[]}\n",
    "\n",
    "#Iterating through the list of titles to scrape the info\n",
    "for i in range(0,len(title_url_list)):\n",
    "        \n",
    "        url = 'https://www.metacritic.com' + title_url_list[i]\n",
    "        response  = requests.get(url, headers = user_agent)\n",
    "        \n",
    "        # Checking that the page exists\n",
    "        if response.status_code == 200: \n",
    "           \n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            url_base = title_url_list[i]\n",
    "            \n",
    "            # CLEANUP - Removing titles with no metacritic score or Canceled\n",
    "            critic_score = soup.find('a', class_='metascore_anchor', href=url_base+'/critic-reviews')\n",
    "            release_date = soup.find('li', class_='summary_detail release_data').find('span', class_='data').text\n",
    "\n",
    "            if critic_score != None and release_date != 'Canceled':\n",
    "\n",
    "                # Today's date, title url, title name, platform & metascore\n",
    "                Today_Date = int(time.strftime(\"%Y%m%d\"))\n",
    "\n",
    "                ea_title_dict['DATE_ID'].append(Today_Date)\n",
    "                ea_title_dict['TITLE_URL'].append(title_url_list[i])\n",
    "                ea_title_dict['TITLE_NAME'].append(titles[i])\n",
    "                ea_title_dict['PLATFORM'].append(platforms[i])\n",
    "                ea_title_dict['METASCORE'].append(int(critic_score.find('span').text))\n",
    "\n",
    "                # publisher_url\n",
    "                publisher_url = soup.find('li', class_='summary_detail publisher').find('a')['href']\n",
    "                ea_title_dict['PUBLISHER_URL'].append(publisher_url)\n",
    "\n",
    "                #average user score\n",
    "                agv_user_score = soup.find('a', class_='metascore_anchor', href=url_base+'/user-reviews')\n",
    "                if agv_user_score == None or agv_user_score.find('div').text == 'tbd':\n",
    "                    ea_title_dict['AGV_USER_SCORE'].append(float(0))\n",
    "                else:\n",
    "                    ea_title_dict['AGV_USER_SCORE'].append(float(agv_user_score.find('div').text))\n",
    "                \n",
    "                #release_date\n",
    "                release_date = soup.find('li', class_='summary_detail release_data')\n",
    "                if release_date == None:\n",
    "                    ea_title_dict['RELEASE_DATE_ID'].append(int(0))\n",
    "                else:\n",
    "                    #exporting the date in the datetime format or keeping the original format for dates that do not fit the format\n",
    "                    try :\n",
    "                        # Date Format 'Oct 13, 2020'\n",
    "                        release_date = datetime.strptime(release_date.find('span', class_='data').text,'%b %d, %Y')\n",
    "                        ea_title_dict['RELEASE_DATE_ID'].append(to_integer(release_date))\n",
    "                    except ValueError:\n",
    "                        try:\n",
    "                            # Date Format '2020'\n",
    "                            release_date = datetime.strptime(release_date.find('span', class_='data').text,'%Y')\n",
    "                            ea_title_dict['RELEASE_DATE_ID'].append(to_integer(release_date))\n",
    "                        except ValueError:\n",
    "                            try:\n",
    "                                # Date Format 'September 2020'\n",
    "                                release_date = datetime.strptime(release_date.find('span', class_='data').text,'%B %Y')\n",
    "                                ea_title_dict['RELEASE_DATE_ID'].append(to_integer(release_date))\n",
    "                            except ValueError:\n",
    "                                # Other Date Formats stored as 0\n",
    "                                ea_title_dict['RELEASE_DATE_ID'].append(int(0))\n",
    "            \n",
    "                #List of genres separated by comma\n",
    "                title_genres_text = ''\n",
    "\n",
    "                # CLEANUP - Reducing the number of genres to 18 (as defined by Metacritic) + Miscellaneous \n",
    "                title_genres_list = ['Action','Adventure','Action Adventure','Fighting','First-Person','Flight','Party','Platformer','Puzzle',\n",
    "                                    'Racing','Real-Time','Role-Playing','Simulation','Sports','Strategy','Third-Person','Turn-Based','Wargames','Wrestling','Miscellaneous']\n",
    "\n",
    "                title_genres = soup.find('li', class_='summary_detail product_genre')\n",
    "                \n",
    "                if title_genres == None:\n",
    "                    ea_title_dict['TITLE_GENRES'].append('')\n",
    "                else:\n",
    "                    for genre in title_genres.find_all('span', class_='data'):\n",
    "                            # CLEANUP - Removing duplicated genre names\n",
    "                            if genre.text in title_genres_list and genre.text not in title_genres_text:\n",
    "                                if title_genres_text == '':\n",
    "                                    title_genres_text = title_genres_text+genre.text\n",
    "                                else:\n",
    "                                    title_genres_text = title_genres_text+','+genre.text\n",
    "                    if title_genres_text == '':\n",
    "                        ea_title_dict['TITLE_GENRES'].append('Miscellaneous')\n",
    "                    else:\n",
    "                        ea_title_dict['TITLE_GENRES'].append(title_genres_text)\n",
    "\n",
    "                #number of critic reviews and number of user reviews\n",
    "                critic_reviews = 0\n",
    "                user_reviews = 0\n",
    "\n",
    "                for summary in soup.find_all('div', class_='summary'):\n",
    "                        if summary.find('a', href=url_base+'/critic-reviews') != None:\n",
    "                            critic_reviews = summary.find('a', href=url_base+'/critic-reviews').find('span').text\n",
    "                            ea_title_dict['CRITIC_REVIEWS_COUNT'].append(int(critic_reviews))\n",
    "                        \n",
    "                        if summary.find('a', href=url_base+'/user-reviews') != None:\n",
    "                            user_reviews = summary.find('a', href=url_base+'/user-reviews').text\n",
    "                            ea_title_dict['USER_RATINGS_COUNT'].append(int(user_reviews.replace(' Ratings','')))\n",
    "\n",
    "                if critic_reviews == 0:\n",
    "                            ea_title_dict['CRITIC_REVIEWS_COUNT'].append(int(0))\n",
    "                if user_reviews == 0:\n",
    "                            ea_title_dict['USER_RATINGS_COUNT'].append(int(0))\n",
    "            \n",
    "                url_base_count = url_base +'/critic-reviews?dist='\n",
    "\n",
    "                #number of positive critic reviews\n",
    "                pos_critic_reviews_count = soup.find('a', href=url_base_count+'positive')\n",
    "                if pos_critic_reviews_count == None:\n",
    "                    ea_title_dict['POS_CRITIC_REVIEWS_COUNT'].append(int(0))\n",
    "                else:\n",
    "                    ea_title_dict['POS_CRITIC_REVIEWS_COUNT'].append(int(pos_critic_reviews_count.find('span', class_=\"count\").text))\n",
    "                \n",
    "                #number of neutral critic reviews\n",
    "                mix_critic_reviews_count = soup.find('a', href=url_base_count+'neutral')\n",
    "                if mix_critic_reviews_count == None:\n",
    "                    ea_title_dict['MIX_CRITIC_REVIEWS_COUNT'].append(int(0))\n",
    "                else:\n",
    "                    ea_title_dict['MIX_CRITIC_REVIEWS_COUNT'].append(int(mix_critic_reviews_count.find('span', class_=\"count\").text))\n",
    "                \n",
    "                #number of negative critic reviews\n",
    "                neg_critic_reviews_count = soup.find('a', href=url_base_count+'negative')\n",
    "                if neg_critic_reviews_count == None:\n",
    "                    ea_title_dict['NEG_CRITIC_REVIEWS_COUNT'].append(int(0))\n",
    "                else:\n",
    "                    ea_title_dict['NEG_CRITIC_REVIEWS_COUNT'].append(int(neg_critic_reviews_count.find('span', class_=\"count\").text))\n",
    "            \n",
    "                url_base_count = url_base +'/user-reviews?dist='\n",
    "                \n",
    "                #number of positive user reviews\n",
    "                pos_user_reviews_count = soup.find('a', href=url_base_count+'positive')\n",
    "                if pos_user_reviews_count == None:\n",
    "                    ea_title_dict['POS_USER_REVIEWS_COUNT'].append(int(0))\n",
    "                else:\n",
    "                    ea_title_dict['POS_USER_REVIEWS_COUNT'].append(int(pos_user_reviews_count.find('span', class_=\"count\").text.replace(',','')))\n",
    "                \n",
    "                #number of neutral user reviews\n",
    "                mix_user_reviews_count = soup.find('a', href=url_base_count+'neutral')\n",
    "                if mix_user_reviews_count == None:\n",
    "                    ea_title_dict['MIX_USER_REVIEWS_COUNT'].append(int(0))\n",
    "                else:\n",
    "                    ea_title_dict['MIX_USER_REVIEWS_COUNT'].append(int(mix_user_reviews_count.find('span', class_=\"count\").text.replace(',','')))\n",
    "                \n",
    "                #number of negative user reviews\n",
    "                neg_user_reviews_count = soup.find('a', href=url_base_count+'negative')\n",
    "                if neg_user_reviews_count == None:\n",
    "                    ea_title_dict['NEG_USER_REVIEWS_COUNT'].append(int(0))\n",
    "                else:\n",
    "                    ea_title_dict['NEG_USER_REVIEWS_COUNT'].append(int(neg_user_reviews_count.find('span', class_=\"count\").text.replace(',','')))\n",
    "\n",
    "            # Progress - % of titles processed - printed in the output bar\n",
    "            clear_output()\n",
    "            print(\"Progress: {:2.1%}\".format(i / len(title_url_list))+\" \")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "# Progress - Completed - printed in the output bar\n",
    "clear_output()\n",
    "#print(\"Complete!\")\n",
    "\n",
    "# OPTIMIZATION : Calculating the time elapsed\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the dictionary into the dataframe\n",
    "ea_title_df = pd.DataFrame(ea_title_dict, columns = ['DATE_ID','PUBLISHER_URL','TITLE_URL', 'TITLE_NAME', 'PLATFORM','TITLE_GENRES', 'RELEASE_DATE_ID',\n",
    "                        'METASCORE', 'CRITIC_REVIEWS_COUNT','POS_CRITIC_REVIEWS_COUNT','MIX_CRITIC_REVIEWS_COUNT','NEG_CRITIC_REVIEWS_COUNT',\n",
    "                        'AGV_USER_SCORE', 'USER_RATINGS_COUNT', 'POS_USER_REVIEWS_COUNT','MIX_USER_REVIEWS_COUNT','NEG_USER_REVIEWS_COUNT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function below is used to break down the titles per row per genre\n",
    "\n",
    "def explode(df, lst_cols, fill_value='', preserve_index=False):\n",
    "    # make sure `lst_cols` is list-alike\n",
    "    if (lst_cols is not None\n",
    "        and len(lst_cols) > 0\n",
    "        and not isinstance(lst_cols, (list, tuple, np.ndarray, pd.Series))):\n",
    "        lst_cols = [lst_cols]\n",
    "    # all columns except `lst_cols`\n",
    "    idx_cols = df.columns.difference(lst_cols)\n",
    "    # calculate lengths of lists\n",
    "    lens = df[lst_cols[0]].str.len()\n",
    "    # preserve original index values    \n",
    "    idx = np.repeat(df.index.values, lens)\n",
    "    # create \"exploded\" DF\n",
    "    res = (pd.DataFrame({\n",
    "                col:np.repeat(df[col].values, lens)\n",
    "                for col in idx_cols},\n",
    "                index=idx)\n",
    "             .assign(**{col:np.concatenate(df.loc[lens>0, col].values)\n",
    "                            for col in lst_cols}))\n",
    "    # append those rows that have empty lists\n",
    "    if (lens == 0).any():\n",
    "        # at least one list in cells is empty\n",
    "        res = (res.append(df.loc[lens==0, idx_cols], sort=False)\n",
    "                  .fillna(fill_value))\n",
    "    # revert the original index order\n",
    "    res = res.sort_index()\n",
    "    # reset index if requested\n",
    "    if not preserve_index:        \n",
    "        res = res.reset_index(drop=True)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the column 'titles_genres from string into a list\n",
    "for i in range (0, len(ea_title_df)):\n",
    "    ea_title_df['TITLE_GENRES'][i] = list(ea_title_df['TITLE_GENRES'][i].split(\",\"))\n",
    "\n",
    "# Apply function above on the df\n",
    "ea_title_df = explode(ea_title_df, ['TITLE_GENRES'], fill_value='', preserve_index=True)\n",
    "ea_title_df = ea_title_df[['DATE_ID','PUBLISHER_URL','TITLE_URL', 'TITLE_NAME', 'PLATFORM','TITLE_GENRES', 'RELEASE_DATE_ID',\n",
    "                        'METASCORE', 'CRITIC_REVIEWS_COUNT','POS_CRITIC_REVIEWS_COUNT','MIX_CRITIC_REVIEWS_COUNT','NEG_CRITIC_REVIEWS_COUNT',\n",
    "                        'AGV_USER_SCORE', 'USER_RATINGS_COUNT', 'POS_USER_REVIEWS_COUNT','MIX_USER_REVIEWS_COUNT','NEG_USER_REVIEWS_COUNT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Initiating login request with your identity provider. A browser window should have opened for you to complete the login. If you can't see it, check existing browser windows, or your OS settings. Press CTRL+C to abort and try again...\n"
    }
   ],
   "source": [
    "# Importing the required packages for all your data framing needs.\n",
    "import pandas as pd\n",
    "\n",
    "# The Snowflake Connector library.\n",
    "import snowflake.connector as snow\n",
    "from snowflake.connector.pandas_tools import write_pandas\n",
    "\n",
    "## Phase I: Truncate/Delete the current data in the table\n",
    "# The connector...\n",
    "conn = snow.connect(user=\"LCUZACOV@EA.COM\",\n",
    "   authenticator='externalbrowser',\n",
    "   account=\"eagai.us-east-1\",\n",
    "   role=\"SUPERUSERS_GAI\",\n",
    "   # (the prefix in your snowflake space... for example, \n",
    "   # company.snowflakecomputing.com would just be \"company\" as the ACCOUNT name)\n",
    "   warehouse=\"QUERY\",\n",
    "   database= \"GAITWAY_SANDBOX\",\n",
    "   schema=\"BF5_SANDBOX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(True,\n 1,\n 1151,\n [('xwumj/file0.txt', 'LOADED', 1151, 1151, 1, 0, None, None, None, None)])"
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "cur = conn.cursor()\n",
    "sql = \"use role SUPERUSERS_GAI\"\n",
    "cur.execute(sql)\n",
    "sql = \"use database GAITWAY_SANDBOX\"\n",
    "cur.execute(sql)\n",
    "sql = \"use schema BF5_SANDBOX\"\n",
    "cur.execute(sql)\n",
    "\n",
    "cur = conn.cursor()\n",
    "\n",
    "sql = \"truncate table if exists LCU_EA_METACRITIC\"\n",
    "cur.execute(sql)\n",
    "\n",
    "#Close the cursor.\n",
    "cur.close()\n",
    "write_pandas(conn, ea_title_df, \"LCU_EA_METACRITIC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exporting the the dataframe into an CSV file with the date and time in the title\n",
    "#Date_Time = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "#excelfilename = 'ea_metacritic_'+Date_Time +\".csv\"\n",
    "#ea_title_df.to_csv (r'/Users/liuba/Desktop/files/'+excelfilename, index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exporting the the dataframe into an EXCEL file with the date and time in the title\n",
    "#Date_Time = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "#excelfilename = 'ea_metacritic_'+Date_Time +\".xlsx\"\n",
    "#ea_title_df.to_excel (r'/Users/liuba/Desktop/files/'+excelfilename, index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}